{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BatchNormalization**:\n",
    "Нормализация входного слоя нейронной сети обычно выполняется путем масштабирования данных, подаваемых в функции активации. Например, когда есть признаки со значениями от 0 до 1 и некоторые признаки со значениями от 1 до 1000, то их необходимо нормализовать, чтобы ускорить обучение. Нормализацию данных можно выполнить и в скрытых слоях нейронных сетей, что и делает метод пакетной нормализации.\n",
    "\n",
    "Но для скрытых слоев нейронной сети такой метод не подходит, так как распределение входных данных для каждого узла скрытых слоев изменяется каждый раз, когда происходит обновление параметров в предыдущем слое. Эта проблема называется **внутренним ковариантным сдвигом** (англ. internal covariate shift). Для решения данной проблемы часто приходится использовать низкий темп обучения (англ. learning rate) и методы регуляризации при обучении модели. Другим способом устранения внутреннего ковариантного сдвига является метод **пакетной нормализации**.\n",
    "\n",
    "**Свойства пакетной нормализации**\n",
    "\n",
    "Кроме того, использование пакетной нормализации обладает еще несколькими дополнительными полезными свойствами:\n",
    "\n",
    "* достигается более быстрая сходимость моделей, несмотря на выполнение дополнительных вычислений;\n",
    "* пакетная нормализация позволяет каждому слою сети обучаться более независимо от других слоев;\n",
    "* становится возможным использование более высокого темпа обучения, так как пакетная нормализация гарантирует, что выходы узлов нейронной сети не будут иметь слишком больших или малых значений;\n",
    "* пакетная нормализация в каком-то смысле также является механизмом регуляризации: данный метод привносит в выходы узлов скрытых слоев некоторый шум, аналогично методу dropout;\n",
    "* модели становятся менее чувствительны к начальной инициализации весов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описание метода:\n",
    "Пусть на вход некоторому слою нейронной сети поступает вектор размерности $d$: $x = (x^{(1)},...,x^{(d)})$. Нормализуем данный вектор по каждой размерности $k$:\n",
    "$\\hat{x}^{(k)} = \\frac{x^{(k)} - E(x^{(k)})}{\\sqrt{D(x^{(k)})}}$,\n",
    "где $E(x^{(k)})$ и $D(x^{(k)})$ высчитываются по всей обучающей выборке.\n",
    "Такая нормализация входа слоя нейронной сети может изменить представление данных в слое. Чтобы избежать данной проблемы, вводятся два параметра сжатия и сдвига нормализованной величины для каждого $x^{(k)}: \\gamma^{(k)}, \\beta^{(k)}$ — которые действуют следующим образом: \n",
    "\n",
    "$y^{(k)} = \\gamma^{(k)}\\hat{x}^{(k)} + \\beta^{(k)}$\n",
    "\n",
    "Данные параметры настраиваются в процессе обучения вместе с остальными параметрами модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

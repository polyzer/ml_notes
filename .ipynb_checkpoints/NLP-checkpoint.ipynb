{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets: \n",
    "[RuSentiment](https://github.com/text-machine-lab/rusentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GPT-2** (The Generative Pre-Trained Transformer from OpenAI):\n",
    "\n",
    "[GPT-2 BERT TransformersXL](http://jalammar.github.io/illustrated-gpt2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT (Bidirectional Encoder Representations from Transformers):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Определение:**\n",
    "**Word2vec** — инструмент для анализа семантики естественных языков, основанный на дистрибутивной семантике, машинном обучении и векторном представлении слов. \n",
    "\n",
    "В word2vec реализованы два основных алгоритма обучения: CBoW (англ. Continuous Bag of Words, «непрерывный мешок со словами», англ. bag — мультимножество) и Skip-gram. **CBoW** — архитектура, которая предсказывает текущее слово, исходя из окружающего его контекста. \n",
    "\n",
    "Архитектура типа **Skip-gram** действует наоборот: она использует текущее слово, чтобы предугадывать окружающие его слова. Пользователь word2vec имеет возможность переключаться и выбирать между алгоритмами. Порядок слов контекста не оказывает влияния на результат ни в одном из этих алгоритмов.\n",
    "\n",
    "Вложения для слов вычисляются по окружающим словам, которые чаще появляются рядом. Механика такая:\n",
    "\n",
    "Получаем много текстовых данных (скажем, все статьи Википедии)\n",
    "Устанавливаем окно (например, из трёх слов), которое скользит по всему тексту.\n",
    "Скользящее окно генерирует образцы для обучения нашей модели\n",
    "\n",
    "**Обучение:**\n",
    "Начнём с первого образца в нашем наборе данных. Берём признак и отправляем его в необученную модель с просьбой предсказать соседнее слово.\n",
    "\n",
    "Модель проходит три шага и выводит вектор предсказания (с вероятностью для каждого слова в словаре). Поскольку модель не обучена, на данном этапе её прогноз наверняка неправильный. Но это ничего. Мы знаем, какое слово она спрогнозирует — это результирующая ячейка в строке, которую мы в настоящее время используем для обучения модели:\n",
    "«Целевой вектор» — тот, в котором у целевого слова вероятность 1, а у всех остальных слов вероятность 0\n",
    "\n",
    "Насколько ошиблась модель? Вычитаем вектор прогноза из целевого и получаем вектор ошибки:\n",
    "Этот вектор ошибки теперь можно использовать для обновления модели, поэтому в следующий раз она с большей вероятностью выдаст точный результат на тех же входных данных.\n",
    "\n",
    "Здесь завершается первый этап обучения. Продолжаем делать то же самое со следующим образцом в наборе данных, а затем со следующим, пока не рассмотрим все образцы. Это конец первой эпохи обучения. Повторяем всё снова и снова в течение нескольких эпох, и в итоге получаем обученную модель: из неё можно извлечь матрицу вложений и использовать в любых приложениях.\n",
    "\n",
    "Хотя мы многое узнали, но для полного понимания, как реально обучается word2vec, не хватает пары ключевых идей.\n",
    "\n",
    "Отрицательный отбор\n",
    "Вспомним три этапа, как нейронная модель вычисляет прогноз:\n",
    "\n",
    "Третий шаг очень дорог с вычислительной точки зрения, особенно если делать его для каждой выборки в наборе данных (десятки миллионов раз). Нужно как-то повысить производительность.\n",
    "\n",
    "Один из способов — разделить цель на два этапа:\n",
    "\n",
    "Создать высококачественные вложения слов (без прогноза следующего слова).\n",
    "Использовать эти высококачественные вложения для обучения языковой модели (для прогнозирования).\n",
    "\n",
    "В этой статье сосредоточимся на первом шаге. Для увеличения производительности можно отойти от прогнозирования соседнего слова…\n",
    "… и переключиться на модель, которая берёт входное и выходное слова и вычисляет вероятность их соседства (от 0 до 1).\n",
    "Такой простой переход заменяет нейронную сеть на модель логистической регрессии — таким образом, вычисления становятся намного проще и быстрее.\n",
    "\n",
    "При этом требуется доработка структуры нашего набора данных: метка теперь является новым столбцом со значениями 0 или 1. В нашей таблице везде единицы, потому что мы добавляли туда соседей.\n",
    "\n",
    "Такая модель вычисляется с невероятной скоростью: миллионы образцов за считанные минуты. Но нужно закрыть одну лазейку. Если все наши примеры положительные (цель: 1), то может образоваться хитрая модель, которая всегда возвращает 1, демонстрируя точность 100%, но она ничему не обучается и генерирует мусорные вложения.\n",
    "\n",
    "Чтобы решить эту проблему, нужно ввести в набор данных отрицательные образцы — слова, которые точно не являются соседями. Для них модель обязана вернуть 0. Теперь модели придётся упорно работать, но вычисления по-прежнему идут на огромной скорости.\n",
    "\n",
    "Эта идея родилась под влиянием метода шумосопоставительного оценивания [pdf]. Мы сопоставляем фактический сигнал (положительные примеры соседних слов) с шумом (случайно выбранные слова, которые не являются соседями). Это обеспечивает отличный компромисс между производительностью и статистической эффективностью.\n",
    "\n",
    "Мы рассмотрели две центральные концепции word2vec: вместе они называются «skip-gram с отрицательной выборкой».\n",
    "\n",
    "**Обучение word2vec**\n",
    "\n",
    "Разобрав основные идеи skip-gram и отрицательной выборки, можем перейти к более пристальному рассмотрению процесса обучения word2vec.\n",
    "\n",
    "Сначала предварительно обрабатываем текст, на котором обучаем модель. Определим размер словаря (будем называть его vocab_size), скажем, в 10 000 вложений и параметры слов в словаре.\n",
    "\n",
    "В начале обучения создаём две матрицы: Embedding и Context. В этих матрицах хранятся вложения для каждого слова в нашем словаре (поэтому vocab_size является одним из их параметров). Второй параметр — размерность вложения (обычно embedding_size устанавливают на 300, но ранее мы рассматривали пример с 50 измерениями).\n",
    "\n",
    "Сначала инициализируем эти матрицы случайными значениями. Затем начинаем процесс обучения. На каждом этапе берём один положительный пример и связанные с ним отрицательные. Вот наша первая группа:\n",
    "\n",
    "Теперь у нас четыре слова: входное слово not и выходные/контекстные слова thou (фактический сосед), aaron и taco (отрицательные примеры). Начинаем поиск их вложений в матрицах Embedding (для входного слова) и Context(для контекстных слов), хотя в обеих матрицах есть вложения для всех слов из нашего словаря.\n",
    "\n",
    "Затем вычисляем скалярное произведение входного вложения с каждым из контекстных вложений. В каждом случае получается число, которое указывает на сходство входных данных и контекстных вложений.\n",
    "\n",
    "Теперь нужен способ превратить эти оценки в некое подобие вероятностей: все они должны быть положительными числами между 0 и 1. Это отличная задача для логистических уравнений sigmoid.\n",
    "\n",
    "Результат вычисления sigmoid можно считать выдачей модели по этим образцам. Как видите, у taco самый высокий балл, а у aaron по-прежнему самая низкая оценка как до, так и после sigmoid.\n",
    "\n",
    "Когда необученная модель сделала прогноз и имея реальную целевую метку для сравнения, давайте посчитаем, сколько ошибок в прогнозе модели. Для этого просто вычитаем оценку sigmoid из целевых меток."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Word2Vec в картинках](https://habr.com/ru/post/446530/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
